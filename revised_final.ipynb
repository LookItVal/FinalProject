{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification of Birds based on audio of their call**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § I: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird species classification based on audio recordings has significant implications for ecological monitoring, biodiversity studies, and conservation efforts, as well as being applicable and interesting to a population of bird watchers and nature lovers. This project focuses on developing a robust machine learning pipeline capable of identifying bird species from basic metadata recordable by a cell phone recording. By leveraging advanced audio signal processing techniques and modern classification algorithms, the project aims to create a reliable and scalable solution for bird call recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Overview of project goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objectives of this project are:\n",
    "\n",
    "### 1. Data Collection and Integration:\n",
    "Analyze, preprocess, and merge audio datasets containing bird calls to create a comprehensive, high-quality dataset suitable for training machine learning models.\n",
    "### 2. Feature Engineering:\n",
    "Extract meaningful features from raw audio signals, including frequency-domain representations, time-frequency representations, and repcrocess the signal to make the target sound the loudest part of the signal.\n",
    "### 3. Model Training and Evaluation:\n",
    "Train and compare multiple classification models, including neural networks and traditional classification algorithms, to identify the most effective approach for bird call classification.\n",
    "### 4. Practical Application:\n",
    "Build a functional tool capable of processing new audio files to classify bird species throughout an audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § II. Data Preperation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building Virtual Enviroment\n",
    "The first step was to establish a virtual enviroment with the required libraries. A virtual environment was created with `micromamba`  to ensure that all dependencies, libraries, and tools used in the project are isolated from the host system. This approach prevents version conflicts and facilitates seamless collaboration and deployment.\n",
    "\n",
    "```bash\n",
    "micromamba create ./.conda conda pydub ffmpeg cupy pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing Libraries\n",
    "From these libraries, and from some of the standard library we can now import the specific parts of the library that we need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupyx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcufft\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupyx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcusig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import time, os, asyncio, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cupy as cp\n",
    "import cupyx.scipy.fft as cufft\n",
    "import cupyx.scipy.signal as cusig\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Overview of Libraries and their purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) `pydub`\n",
    "`pydub` is a library used for simple processing of audio files. It provides a simple means for decoding mp3 files in python, and manipulating the audio data in a relatively basic way. `pydub` provides the ability to split up audio files as if they were python lists where the index of the list is the number of miliseconds from the begining of the file. With the addition of another library called `simpleaudio` audio can be played directly with python, but for this project all audio files will exported and displayed with html in markdown cells.\n",
    "```python\n",
    "# loads audio file\n",
    "sound = AudioSegment.from_file(filename, format='wav')\n",
    "\n",
    "# cuts the sound to only the first second of the file\n",
    "sound = sound[:1000]\n",
    "\n",
    "# returns the signal array for more nuanced signal manipulation\n",
    "signal = sound.get_array_of_samples()\n",
    "\n",
    "# returns a list of mono sounds for each channel in the audio file\n",
    "sounds = sound.split_to_mono()\n",
    "```\n",
    "Metadata of audio files is also accessed with pydub either through the `pydub.AudioSegment` object or through the `pydub.utils.mediainfo` function.\n",
    "```python\n",
    "# the length of the audio file in miliseconds\n",
    "len(sound)\n",
    "\n",
    "# the sample rate of the audio file\n",
    "sound.frame_rate\n",
    "\n",
    "# the number of channels in the audio file\n",
    "sound.channels\n",
    "\n",
    "# returns the name of the format used to store the audio file\n",
    "mediainfo(filename)['format_name']\n",
    "\n",
    "# returns the sample format, needed to get the bit depth of the file\n",
    "mediainfo(filename)['sample_fmt']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) `ffmpeg`\n",
    "`ffmpeg` is a powerful tool for handling a wide variety of media formats, including audio and video. Within this environment, `ffmpeg` serves as the backend for `pydub` to decode `.mp3` files. By default, when an `.mp3` file is loaded using `pydub`, `ffmpeg` is called automatically to handle the decoding. This integration ensures that the project can work seamlessly with compressed audio formats like `.mp3`.\n",
    "```python\n",
    "# decodes codec with ffmpeg and returns the AudioSegment object as normal\n",
    "sound = AudioSegment.from_file(filename, format='mp3')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) `cupy`\n",
    "`cupy` is a GPU-accelerated library that provides a drop-in replacement for the `numpy` and `scipy` libraries, leveraging NVIDIA's CUDA toolkit for GPU-accelerated parallel processing. This allows for significant performance improvements when performing computationally intensive tasks, such as Fourier transformations, matrix operations, and large-scale data manipulations.\n",
    "\n",
    "In this project, `cupy` was particularly useful for processing large audio datasets and accelerating tasks like feature extraction and signal transformations, where CPU-based computation would be too slow.\n",
    "```python\n",
    "import cupy as cp\n",
    "\n",
    "# Create a large random array on the GPU\n",
    "gpu_array = cp.random.rand(1000000)\n",
    "\n",
    "# Perform fast computations on the GPU\n",
    "mean_value = cp.mean(gpu_array)\n",
    "```\n",
    "Arrays in match the syntax of `numpy.ndarray` objects and `scipy` methods can be called via the `cupyx.scipy` library.\n",
    "\n",
    "**CUDA-Specific Features**\n",
    "\n",
    "While `cupy` emulates `numpy` and `scipy` functionality, it also offers CUDA-specific tools to optimize memory and computation. These are critical for managing GPU resources efficiently when working with large datasets or computationally intensive tasks.\n",
    "\n",
    " - **Device Management:** `cupy` supports multi-GPU setups and allows explicit control over which GPU device is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = cp.cuda.Device(0)  # Access the first GPU\n",
    "gpu.use()  # Set it as the active GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Memory Management**: Memory allocation and deallocation are handled through memory pools to reduce overhead during GPU memory operations. The default memory pool can be configured to limit VRAM usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the memory pool limit to 75% of the total VRAM\n",
    "vram_pool = cp.get_default_memory_pool()\n",
    "with gpu:\n",
    "    vram_pool.set_limit(fraction=0.75)\n",
    "\n",
    "# Clear the memory pool\n",
    "vram_pool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This method helps ensure explicit memory errors occur and stop the program speeding up debugging. `cupy` will run without manual allocation of ram it is just more likely to have problems related to memory overflow without explcitly explaining the cause of the progam error.\n",
    "\n",
    " - **FFT Plans with `cupy.cuda.cufft`:** `cupy` provides access to CUDA's FFT library (`cuFFT`), which allows for pre-planning and optimizing FFT operations. Each FFT plan is formed based on the shape of the input array. To improve performance an prevent recalculating FFT plans, the resulting FFT plan is automatically cached in the default memory pool. To perform an FFT without caching the FFT plan, you can manually create a plan and pass the plan into the FFT, or you can clear the cache of the FFT plan whenever you like with a simple syntax.\n",
    "```python\n",
    "with cufft.get_fft_plan(signal, value_type='R2C'):  # Create FFT plan and pass into FFT function\n",
    "    gpu_fft = cufft.rfft(signal, overwrite_x=True)  # This does not cache FFT plan\n",
    "\n",
    "# or clear the cache at any point\n",
    "cp.fft.config.clear_plan_cache()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) `pytorch`\n",
    "`pytorch` is a widely used library for building and training neural networks chosen because of its simple integration with `numpy` arrays. Most of the models we will train in this notebook can be trained with libraries from the anaconda collection but `pytorch` provides an ability to make customizable networks, and networks of different forms, like Convolutional Neural Networks (CNN) and Reccurent Neural Networks (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Establishing Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Simple Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means of loading file that always ensure the correct format\n",
    "load_file = lambda file: AudioSegment.from_file(file, format=mediainfo(file)['format_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Believe it or not, sometimes the file format ends in `.mp3` but the actual internal format is used is that of a `.wav` file. In those cases `mediainfo(file)['format_name']` will return the actual internal format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Data Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a percentage value and maps it to a color in the range of red to green\n",
    "def percentage_color_conversion(input_value: int|float, input_min: int = 0, input_max: int = 100, output_min: int = 0, output_max: int = 120) -> float:\n",
    "    # Normalize the input value to a range of 0 to 1\n",
    "    normalized_input = (input_value - input_min) / (input_max - input_min)\n",
    "    # Apply an exponential function to create a non-linear scale\n",
    "    scaled_value = (np.exp(normalized_input) - 1) / (np.e - 1)\n",
    "    # Repeat the function to create a more pronounced effect\n",
    "    scaled_value = (np.exp(scaled_value) - 1) / (np.e - 1)\n",
    "    # Map the scaled value to the output range\n",
    "    return output_min + scaled_value * (output_max - output_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Human Readable Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a number of bytes to a human readable format of data size\n",
    "def bytes_to_human_readable(n: int, depth: int = 2) -> str:\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if n < 1024:\n",
    "            return f\"{n:.{depth}f} {unit}\"\n",
    "        n /= 1024\n",
    "    return f\"{n:.{depth}f} PB\"\n",
    "\n",
    "# Converts a number of seconds to a human readable format of time\n",
    "def time_to_human_readable(seconds: int|float, depth: int = 3) -> str:\n",
    "    if depth == 0 or depth == 1:\n",
    "        if isinstance(seconds, int):\n",
    "            return f'{seconds} seconds'\n",
    "        miliseconds = (seconds - int(seconds)) * 1000\n",
    "        return f'{int(seconds):.02}:{int(miliseconds):03}'\n",
    "    if depth == 2:\n",
    "        minutes, seconds = divmod(seconds, 60)\n",
    "        return f'{int(minutes):02}:{int(seconds):02}'\n",
    "    hours, rem = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return f'{int(hours):02}:{int(minutes):02}:{int(seconds):02}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Process Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the memory usage of a DataFrame in bytes\n",
    "def memory_usage(df: pd.DataFrame) -> int:\n",
    "    return df.memory_usage(deep=True).sum()\n",
    "\n",
    "# Estimates the time remaining for a process based on the time when it started, the current iteration, and the total number of iterations\n",
    "def calculate_time_remaining(start_time: float, iteration: int, length: int) -> str:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    iterations_remaining = length - iteration\n",
    "    time_per_iteration = elapsed_time / iteration\n",
    "    time_remaining = iterations_remaining * time_per_iteration\n",
    "    time_remaining_str = time_to_human_readable(time_remaining)\n",
    "    return time_remaining_str\n",
    "\n",
    "# Function to print a progress bar that updates in place\n",
    "def print_progress_bar(iteration: int, length: int, display_handler=None, message='', treat_as_data=False) -> DisplayHandle:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        iteration (int): Current iteration or progress value.\n",
    "        length (int): Total length or maximum value for the progress.\n",
    "        display_handler (optional): An existing display handler to update the progress bar. Defaults to None where a new display handler is generated. \n",
    "        message (str, optional): Additional message to display alongside the progress bar.\n",
    "        treat_as_data (bool, optional): If True, converts iteration and length to human-readable data format. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        display_handler: The display handler used to render the progress bar.\n",
    "                         * In order to update the progress bar, pass this handler to the next call of this function.\n",
    "    \n",
    "    Notes:\n",
    "        This function is intended for use in Jupyter notebooks, and uses the IPython display module to render the progress bar as HTML.\n",
    "        That HTML is rendered in place, allowing the progress bar to update in place.\n",
    "    \"\"\"\n",
    "    # Calculate progress as a whole number percentage\n",
    "    progress = iteration / length\n",
    "    progress = int(progress * 100)\n",
    "    # Get the color for the progress bar based on the percentage value using css hsl color format\n",
    "    color = f'hsl({percentage_color_conversion(progress)}, 100%, 50%)'\n",
    "    # Create the progress bar as a string of equal signs with the appropriate color\n",
    "    bar = f'<span style=\"color: {color};\">{\"=\" * progress}</span>{' ' * (100 - progress)}'\n",
    "    # Convert the iteration and length to human-readable data format if specified\n",
    "    if treat_as_data:\n",
    "        iteration = bytes_to_human_readable(iteration)\n",
    "        length = bytes_to_human_readable(length)\n",
    "    # Ensure monospacing for consistent character width\n",
    "    style = 'font-family: monospace; font-size: 13px;'\n",
    "    # Create the message to display text in html format\n",
    "    html_string = f'<pre style=\"{style}\">[{bar}] - {iteration}/{length}'\n",
    "    if message:\n",
    "        html_string += f'- {message}'\n",
    "    html_string += '</pre>'\n",
    "    # display the message as HTML, updating the display handler if provided\n",
    "    if not display_handler:\n",
    "        display_handler = display(HTML(message), display_id=True)\n",
    "    else:\n",
    "        display_handler.update(HTML(message))\n",
    "    # Return the display handler for updating the progress bar\n",
    "    return display_handler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Signal Generation\n",
    "###### (Hopefully what these do is relatively self evident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Sine Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_wave(frequency: int, duration: int|float, sample_rate: int = 44100, amplitude: float = 0.5) -> AudioSegment:\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    waveform = amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "    waveform = np.int16(waveform * 32767)\n",
    "    audio = AudioSegment(\n",
    "        waveform.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=waveform.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    return audio\n",
    "\n",
    "generate_sine_wave(440, 10).export('export/440hz.wav', format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"export/440hz.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element, but this sounds like a digital tone at A4.\n",
    "</audio>\n",
    "\n",
    "##### (ii) Triangle Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triangle_wave(frequency: int, duration: int|float, sample_rate: int = 44100, amplitude: float = 0.5):\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    waveform = 2 * amplitude * np.abs(2 * (t * frequency - np.floor(t * frequency + 0.5))) - amplitude\n",
    "    waveform = np.int16(waveform * 32767)\n",
    "    audio = AudioSegment(\n",
    "        waveform.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=waveform.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    return audio\n",
    "\n",
    "generate_triangle_wave(440, 10).export('export/440hz_triangle.wav', format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"export/440hz_triangle.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element, but this sounds like a digital tone at A4, but this time a little buzzier.\n",
    "</audio>\n",
    "\n",
    "##### (iii) Sawtooth Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sawtooth_wave(frequency: int, duration: int|float, sample_rate: int = 44100, amplitude: float = 0.5) -> AudioSegment:\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    waveform = 2 * amplitude * (t * frequency - np.floor(t * frequency + 0.5))\n",
    "    waveform = np.int16(waveform * 32767)\n",
    "    audio = AudioSegment(\n",
    "        waveform.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=waveform.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    return audio\n",
    "\n",
    "generate_sawtooth_wave(440, 10).export('export/440hz_sawtooth.wav', format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"export/440hz_sawtooth.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element, but this sounds like a digital tone at A4, but this time a little buzzier.\n",
    "</audio>\n",
    "\n",
    "##### (iv) Square Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_wave(frequency: int, duration: int|float, sample_rate: int = 44100, amplitude: float = 0.5) -> AudioSegment:\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    waveform = amplitude * sp.signal.square(2 * np.pi * frequency * t)\n",
    "    waveform = np.int16(waveform * 32767)\n",
    "    audio = AudioSegment(\n",
    "        waveform.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=waveform.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "    return audio\n",
    "\n",
    "generate_square_wave(440, 10).export('export/440hz_square.wav', format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<audio controls>\n",
    "  <source src=\"export/440hz_square.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element, but this sounds like a digital tone at A4, but this time a little buzzier.\n",
    "</audio>\n",
    "\n",
    "\n",
    "##### (v) White Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vi) Pink Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Print Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_memory_usage = lambda df: print(f\"Memory Usage: {bytes_to_human_readable(memory_usage(df))}\")\n",
    "\n",
    "# Just prints a line with the message in the center\n",
    "def print_boundry(message: str) -> None:\n",
    "    length = (100 - len(message)) // 2\n",
    "    print('-' * length, message, '-' * length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Dataset Exploration and Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial Dataset Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Downloading and Viewing Primary Dataset\n",
    "The first data source is from a [kaggle competition](https://www.birds.cornell.edu/clementschecklist/introduction/updateindex/october-2023/download/). This dataset provides preorganized bird audio data, structured as `.mp3` files grouped by bird species. Each species is identified by a unique eBird code, a standard maintained by the Cornell Lab of Ornithology.\n",
    "\n",
    "This can also be downloaded by running this code after accepting the terms of the competition on a valid kaggle account:\n",
    "```bash\n",
    "kaggle competitions download -c birdsong-recognition\n",
    "```\n",
    "The dataset directory structure is as follows:\n",
    "```log\n",
    ".\n",
    "├── example_test_audio\n",
    "│  ├── BLKFR-10-CPL_20190611_093000.pt540.mp3\n",
    "│  └── ORANGE-7-CAP_20190606_093000.pt623.mp3\n",
    "├── example_test_audio_metadata.csv\n",
    "├── example_test_audio_summary.csv\n",
    "├── sample_submission.csv\n",
    "├── test.csv\n",
    "├── train.csv\n",
    "└── train_audio\n",
    "   ├── aldfly\n",
    "   │  ├── XC2628.mp3\n",
    "   │  └── ...\n",
    "   ├── ameavo\n",
    "   │  ├── XC99571.mp3\n",
    "   │  └── ...\n",
    "   └── ...\n",
    "```\n",
    "Key components of the dataset:\n",
    " - `train_audio/`: Audio files organized by bird species (eBird codes as folder names).\n",
    " - `train.csv`: Metadata about each audio file, including eBird codes, audio duration, and recording location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/birdsong-recognition/train.csv')\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset lacks full taxonomy information, which limits the ability to classify birds at higher taxonomic levels (e.g., family or order) which may be easier targets than individual species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Acquisition and Integration of Additional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Acquiring Secondary Dataset and Verifying Dataset Compatibility\n",
    "To supplement the primary dataset, a taxonomy dataset was downloaded from the [Cornell Lab of Ornithology](https://www.birds.cornell.edu/clementschecklist/introduction/updateindex/october-2023/download/). This dataset includes hierarchical taxonomic information for each bird species, such as family, order, and genus, as well as a human understandable species group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = pd.read_csv('data/ebird_taxonomy_v2023.csv')\n",
    "print(taxonomy.columns)\n",
    "taxonomy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure compatibility between datasets:\n",
    " - **Ensuring uniqueness**: Verified that the SPECIES_CODE column in the taxonomy dataset contained unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_species_codes = len(taxonomy['SPECIES_CODE'])\n",
    "unique_species_codes = taxonomy['SPECIES_CODE'].nunique()\n",
    "print('All values in SPECIES_CODE are unique:', total_species_codes == unique_species_codes)\n",
    "del total_species_codes, unique_species_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Ensure Consistent use of ebird code**: Verified that where the `ebird_code` matches between the datasets, the species also matches.\n",
    "\n",
    "###### (This is happening by merging the datasets because it is considerably faster than iterating between the two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the taxonomy data with the training data\n",
    "merged_df = df.merge(taxonomy, left_on='ebird_code', right_on='SPECIES_CODE', how='left', indicator=True)\n",
    "# Get the ebird_codes that did not merge or have a scientific name mismatch between the two datasets\n",
    "bad_codes = merged_df[(merged_df['_merge'] == 'left_only') | (merged_df['sci_name'] != merged_df['SCI_NAME'])]['ebird_code'].unique()\n",
    "\n",
    "# Iterate through the bad codes and print out the scientific names that do not match\n",
    "for code in bad_codes:\n",
    "    if code not in taxonomy['SPECIES_CODE'].values:\n",
    "        print(code, \"not found in taxonomy\")\n",
    "    else:\n",
    "        birdsong_df_species = merged_df[merged_df['ebird_code'] == code]['sci_name'].unique()\n",
    "        taxonomy_species = taxonomy[taxonomy['SPECIES_CODE'] == code]['SCI_NAME'].unique()\n",
    "        if len(birdsong_df_species) == 1 and len(taxonomy_species) == 1:\n",
    "            print(f'{code}: {birdsong_df_species[0]} != {taxonomy_species[0]}')\n",
    "        if len(birdsong_df_species) == 0:\n",
    "            print(f'{code}: No scientific name in training data')\n",
    "        if len(taxonomy_species) == 0:\n",
    "            print(f'{code}: No scientific name in taxonomy data')\n",
    "        if len(birdsong_df_species) > 1:\n",
    "            print(f'{code}: Multiple scientific names in training data')\n",
    "        if len(taxonomy_species) > 1:\n",
    "            print(f'{code}: Multiple scientific names in taxonomy data')\n",
    "\n",
    "del bad_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Merging The Datasets\n",
    "With compatibility established, the taxonomy dataset was integrated into the primary dataset. The original species information from the primary dataset was replaced with the taxonomy dataset’s more comprehensive information. Key benefits of this approach:\n",
    "\n",
    "Access to taxonomic hierarchies (e.g., order, family, genus) for flexible classification.\n",
    "Improved consistency and expanded metadata for future feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df\n",
    "del merged_df\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Removing Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['_merge', 'playback_used', 'pitch', 'speed',\n",
    "                      'species', 'number_of_notes', 'title', 'secondary_labels',\n",
    "                      'bird_seen', 'sci_name', 'location', 'description',\n",
    "                      'bitrate_of_mp3', 'volume', 'background', 'xc_id',\n",
    "                      'url', 'country', 'author', 'primary_label',\n",
    "                      'length', 'recordist', 'license', 'TAXON_ORDER',\n",
    "                      'CATEGORY', 'SPECIES_CODE', 'TAXON_CONCEPT_ID', 'REPORT_AS'])\n",
    "df.rename(columns={'PRIMARY_COM_NAME': 'common_name', \n",
    "                   'SCI_NAME': 'sci_name', \n",
    "                   'ORDER': 'order', \n",
    "                   'FAMILY': 'family', \n",
    "                   'SPECIES_GROUP': 'species_group'}, inplace=True)\n",
    "print(df.columns)\n",
    "memory_usage('Merge Data', df)\n",
    "del taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Removing Problematic Sample Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Audio Metadata Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Normalizing Date and Time values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Normalizing Latitude, Longitude and Elevation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Final Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Explanation for Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Explaination for Target Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('common_name:', df['common_name'].nunique())\n",
    "print('sci_name:', df['sci_name'].nunique())\n",
    "print('order:', df['order'].nunique())\n",
    "print('family:', df['family'].nunique())\n",
    "print('species_group:', df['species_group'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Explination for Remaining Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § IV. Audio Processing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Audio Metering and Signal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding Different Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Decibel (dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Frequency Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Mel Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Bark Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Equivalent Rectangular Bandwidth Scale (ERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) Visualizing Different Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plotting Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) RMS Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understanding Different Signal Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) The Fourier Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Understanding The Complex Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Advantage of GPU Parallelization when Performing Fourier Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Grouping Frequencies Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) Example Frequency Spectrums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) The Short-Time Fourier Transformation (STFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Advantage of GPU Parallelization when Performing Short-Time Fourier Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) The Inverse Short-Time Fourier Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) The Constant OverLap Add (COLA) Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) The Nonzero OverLap Add (NOLA) Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (v) Grouping Frequencies Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vi) Example Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) The Hilbert Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Understanding the Analytic Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Advantage of GPU Parallelization when Performing Hilbert Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Extracting the Real Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) Extracting the Hilbert Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (v) Extracting the Instantaneous Phase Angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vi) Example Plots of Hilbert Envelopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vii) Example Plots of Signal Phase Angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Generation of Simple Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Performing the Bulk Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualizations of Resulting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Individual Frequency Spectrums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Heatmaps of Frequency Spectrums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Audio Signal Processing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reduction of the Noise Floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Issues Caused by Noise Floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Method of Noise Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Noise Reduction Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reduction of Momentary Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Issued Caused by Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Means of Detecting Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Peak Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Calcuating the Click-Sensitive Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Reducing Magnitude around Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retuction Of Transient Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Introduction to Signal Transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Issues Caused by Transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Calculating the Transient-Sensitive Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Reducing Magnitude around Transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Segmentation of Audio into Normalized Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Generation of Filtered Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Generation of Filtered Time-Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § V. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Filtered Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § V. Advanced Model Training and Species Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Training a Convolutional Neural Network (CNN) Against the Filtered Time-Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Model Training with Species as a Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Training a Convolutional Neural Network (CNN) with Species as a Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § VI. Practical Application and Use of Given Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Recreating the Audio Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Making the Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Testing Against the Given Test Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
