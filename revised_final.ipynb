{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification of Birds based on audio of their call**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง I: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird species classification based on audio recordings has significant implications for ecological monitoring, biodiversity studies, and conservation efforts, as well as being applicable and interesting to a population of bird watchers and nature lovers. This project focuses on developing a robust machine learning pipeline capable of identifying bird species from basic metadata recordable by a cell phone recording. By leveraging advanced audio signal processing techniques and modern classification algorithms, the project aims to create a reliable and scalable solution for bird call recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Overview of project goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objectives of this project are:\n",
    "\n",
    "### 1. Data Collection and Integration:\n",
    "Analyze, preprocess, and merge audio datasets containing bird calls to create a comprehensive, high-quality dataset suitable for training machine learning models.\n",
    "### 2. Feature Engineering:\n",
    "Extract meaningful features from raw audio signals, including frequency-domain representations, time-frequency representations, and repcrocess the signal to make the target sound the loudest part of the signal.\n",
    "### 3. Model Training and Evaluation:\n",
    "Train and compare multiple classification models, including neural networks and traditional classification algorithms, to identify the most effective approach for bird call classification.\n",
    "### 4. Practical Application:\n",
    "Build a functional tool capable of processing new audio files to classify bird species throughout an audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง II. Data Preperation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building Virtual Enviroment\n",
    "The first step was to establish a virtual enviroment with the required libraries. A virtual environment was created with `micromamba`  to ensure that all dependencies, libraries, and tools used in the project are isolated from the host system. This approach prevents version conflicts and facilitates seamless collaboration and deployment.\n",
    "\n",
    "```bash\n",
    "micromamba create ./.conda conda pydub ffmpeg cupy pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing Libraries\n",
    "From these libraries, and from some of the standard library we can now import the specific parts of the library that we need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupyx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcufft\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupyx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcusig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import time, os, asyncio, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cupy as cp\n",
    "import cupyx.scipy.fft as cufft\n",
    "import cupyx.scipy.signal as cusig\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Overview of Libraries and their purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) `pydub`\n",
    "`pydub` is a library used for simple processing of audio files. It provides a simple means for decoding mp3 files in python, and manipulating the audio data in a relatively basic way. `pydub` provides the ability to split up audio files as if they were python lists where the index of the list is the number of miliseconds from the begining of the file. With the addition of another library called `simpleaudio` audio can be played directly with python, but for this project all audio files will exported and displayed with html in markdown cells.\n",
    "```python\n",
    "# loads audio file\n",
    "sound = AudioSegment.from_file(filename, format='wav')\n",
    "\n",
    "# cuts the sound to only the first second of the file\n",
    "sound = sound[:1000]\n",
    "\n",
    "# returns the signal array for more nuanced signal manipulation\n",
    "signal = sound.get_array_of_samples()\n",
    "\n",
    "# returns a list of mono sounds for each channel in the audio file\n",
    "sounds = sound.split_to_mono()\n",
    "```\n",
    "Metadata of audio files is also accessed with pydub either through the `pydub.AudioSegment` object or through the `pydub.utils.mediainfo` function.\n",
    "```python\n",
    "# the length of the audio file in miliseconds\n",
    "len(sound)\n",
    "\n",
    "# the sample rate of the audio file\n",
    "sound.frame_rate\n",
    "\n",
    "# the number of channels in the audio file\n",
    "sound.channels\n",
    "\n",
    "# returns the name of the format used to store the audio file\n",
    "mediainfo(filename)['format_name']\n",
    "\n",
    "# returns the sample format, needed to get the bit depth of the file\n",
    "mediainfo(filename)['sample_fmt']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) `ffmpeg`\n",
    "`ffmpeg` is a powerful tool for handling a wide variety of media formats, including audio and video. Within this environment, `ffmpeg` serves as the backend for `pydub` to decode `.mp3` files. By default, when an `.mp3` file is loaded using `pydub`, `ffmpeg` is called automatically to handle the decoding. This integration ensures that the project can work seamlessly with compressed audio formats like `.mp3`.\n",
    "```python\n",
    "# decodes codec with ffmpeg and returns the AudioSegment object as normal\n",
    "sound = AudioSegment.from_file(filename, format='mp3')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) `cupy`\n",
    "`cupy` is a GPU-accelerated library that provides a drop-in replacement for the `numpy` and `scipy` libraries, leveraging NVIDIA's CUDA toolkit for GPU-accelerated parallel processing. This allows for significant performance improvements when performing computationally intensive tasks, such as Fourier transformations, matrix operations, and large-scale data manipulations.\n",
    "\n",
    "In this project, `cupy` was particularly useful for processing large audio datasets and accelerating tasks like feature extraction and signal transformations, where CPU-based computation would be too slow.\n",
    "```python\n",
    "import cupy as cp\n",
    "\n",
    "# Create a large random array on the GPU\n",
    "gpu_array = cp.random.rand(1000000)\n",
    "\n",
    "# Perform fast computations on the GPU\n",
    "mean_value = cp.mean(gpu_array)\n",
    "```\n",
    "Arrays in match the syntax of `numpy.ndarray` objects and `scipy` methods can be called via the `cupyx.scipy` library.\n",
    "\n",
    "**CUDA-Specific Features**\n",
    "\n",
    "While `cupy` emulates `numpy` and `scipy` functionality, it also offers CUDA-specific tools to optimize memory and computation. These are critical for managing GPU resources efficiently when working with large datasets or computationally intensive tasks.\n",
    "\n",
    " - **Device Management:** `cupy` supports multi-GPU setups and allows explicit control over which GPU device is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = cp.cuda.Device(0)  # Access the first GPU\n",
    "gpu.use()  # Set it as the active GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Memory Management**: Memory allocation and deallocation are handled through memory pools to reduce overhead during GPU memory operations. The default memory pool can be configured to limit VRAM usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the memory pool limit to 75% of the total VRAM\n",
    "vram_pool = cp.get_default_memory_pool()\n",
    "with gpu:\n",
    "    vram_pool.set_limit(fraction=0.75)\n",
    "\n",
    "# Clear the memory pool\n",
    "vram_pool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This method helps ensure explicit memory errors occur and stop the program speeding up debugging. `cupy` will run without manual allocation of ram it is just more likely to have problems related to memory overflow without explcitly explaining the cause of the progam error.\n",
    "\n",
    " - **FFT Plans with `cupy.cuda.cufft`:** `cupy` provides access to CUDA's FFT library (`cuFFT`), which allows for pre-planning and optimizing FFT operations. Each FFT plan is formed based on the shape of the input array. To improve performance an prevent recalculating FFT plans, the resulting FFT plan is automatically cached in the default memory pool. To perform an FFT without caching the FFT plan, you can manually create a plan and pass the plan into the FFT, or you can clear the cache of the FFT plan whenever you like with a simple syntax.\n",
    "```python\n",
    "with cufft.get_fft_plan(signal, value_type='R2C'):  # Create FFT plan and pass into FFT function\n",
    "    gpu_fft = cufft.rfft(signal, overwrite_x=True)  # This does not cache FFT plan\n",
    "\n",
    "# or clear the cache at any point\n",
    "cp.fft.config.clear_plan_cache()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) `pytorch`\n",
    "`pytorch` is a widely used library for building and training neural networks chosen because of its simple integration with `numpy` arrays. Most of the models we will train in this notebook can be trained with libraries from the anaconda collection but `pytorch` provides an ability to make customizable networks, and networks of different forms, like Convolutional Neural Networks (CNN) and Reccurent Neural Networks (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Dataset Exploration and Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial Dataset Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Downloading and Viewing Primary Dataset\n",
    "The first data source is from a [kaggle competition](https://www.birds.cornell.edu/clementschecklist/introduction/updateindex/october-2023/download/). This dataset provides preorganized bird audio data, structured as `.mp3` files grouped by bird species. Each species is identified by a unique eBird code, a standard maintained by the Cornell Lab of Ornithology.\n",
    "\n",
    "This can also be downloaded by running this code after accepting the terms of the competition on a valid kaggle account:\n",
    "```bash\n",
    "kaggle competitions download -c birdsong-recognition\n",
    "```\n",
    "The dataset directory structure is as follows:\n",
    "```log\n",
    ".\n",
    "โโโ example_test_audio\n",
    "โ  โโโ BLKFR-10-CPL_20190611_093000.pt540.mp3\n",
    "โ  โโโ ORANGE-7-CAP_20190606_093000.pt623.mp3\n",
    "โโโ example_test_audio_metadata.csv\n",
    "โโโ example_test_audio_summary.csv\n",
    "โโโ sample_submission.csv\n",
    "โโโ test.csv\n",
    "โโโ train.csv\n",
    "โโโ train_audio\n",
    "   โโโ aldfly\n",
    "   โ  โโโ XC2628.mp3\n",
    "   โ  โโโ ...\n",
    "   โโโ ameavo\n",
    "   โ  โโโ XC99571.mp3\n",
    "   โ  โโโ ...\n",
    "   โโโ ...\n",
    "```\n",
    "Key components of the dataset:\n",
    " - `train_audio/`: Audio files organized by bird species (eBird codes as folder names).\n",
    " - `train.csv`: Metadata about each audio file, including eBird codes, audio duration, and recording location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/birdsong-recognition/train.csv')\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset lacks full taxonomy information, which limits the ability to classify birds at higher taxonomic levels (e.g., family or order) which may be easier targets than individual species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Acquisition and Integration of Additional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Acquiring Secondary Dataset and Verifying Dataset Compatibility\n",
    "To supplement the primary dataset, a taxonomy dataset was downloaded from the [Cornell Lab of Ornithology](https://www.birds.cornell.edu/clementschecklist/introduction/updateindex/october-2023/download/). This dataset includes hierarchical taxonomic information for each bird species, such as family, order, and genus, as well as a human understandable species group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = pd.read_csv('data/ebird_taxonomy_v2023.csv')\n",
    "print(taxonomy.columns)\n",
    "taxonomy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure compatibility between datasets:\n",
    " - **Ensuring uniqueness**: Verified that the SPECIES_CODE column in the taxonomy dataset contained unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_species_codes = len(taxonomy['SPECIES_CODE'])\n",
    "unique_species_codes = taxonomy['SPECIES_CODE'].nunique()\n",
    "print('All values in SPECIES_CODE are unique:', total_species_codes == unique_species_codes)\n",
    "del total_species_codes, unique_species_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Ensure Consistent use of ebird code**: Verified that where the `ebird_code` matches between the datasets, the species also matches.\n",
    "\n",
    "###### (This is happening by merging the datasets because it is considerably faster than iterating between the two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the taxonomy data with the training data\n",
    "merged_df = df.merge(taxonomy, left_on='ebird_code', right_on='SPECIES_CODE', how='left', indicator=True)\n",
    "# Get the ebird_codes that did not merge or have a scientific name mismatch between the two datasets\n",
    "bad_codes = merged_df[(merged_df['_merge'] == 'left_only') | (merged_df['sci_name'] != merged_df['SCI_NAME'])]['ebird_code'].unique()\n",
    "\n",
    "# Iterate through the bad codes and print out the scientific names that do not match\n",
    "for code in bad_codes:\n",
    "    if code not in taxonomy['SPECIES_CODE'].values:\n",
    "        print(code, \"not found in taxonomy\")\n",
    "    else:\n",
    "        birdsong_df_species = merged_df[merged_df['ebird_code'] == code]['sci_name'].unique()\n",
    "        taxonomy_species = taxonomy[taxonomy['SPECIES_CODE'] == code]['SCI_NAME'].unique()\n",
    "        if len(birdsong_df_species) == 1 and len(taxonomy_species) == 1:\n",
    "            print(f'{code}: {birdsong_df_species[0]} != {taxonomy_species[0]}')\n",
    "        if len(birdsong_df_species) == 0:\n",
    "            print(f'{code}: No scientific name in training data')\n",
    "        if len(taxonomy_species) == 0:\n",
    "            print(f'{code}: No scientific name in taxonomy data')\n",
    "        if len(birdsong_df_species) > 1:\n",
    "            print(f'{code}: Multiple scientific names in training data')\n",
    "        if len(taxonomy_species) > 1:\n",
    "            print(f'{code}: Multiple scientific names in taxonomy data')\n",
    "\n",
    "del merged_df, bad_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Merging The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Removing Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Removing Problematic Sample Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Audio Metadata Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Normalizing Date and Time values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Normalizing Latitude, Longitude and Elevation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Final Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Explanation for Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Explaination for Target Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Explination for Remaining Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง III. Establishing Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple Aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Data Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Process Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง IV. Audio Processing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Audio Metering and Signal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding Different Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Decibel (dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Frequency Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Mel Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Bark Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Equivalent Rectangular Bandwidth Scale (ERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) Visualizing Different Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plotting Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) RMS Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understanding Different Signal Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) The Fourier Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Understanding The Complex Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Advantage of GPU Parallelization when Performing Fourier Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Grouping Frequencies Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) Example Frequency Spectrums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) The Short-Time Fourier Transformation (STFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Advantage of GPU Parallelization when Performing Short-Time Fourier Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) The Inverse Short-Time Fourier Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) The Constant OverLap Add (COLA) Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) The Nonzero OverLap Add (NOLA) Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (v) Grouping Frequencies Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vi) Example Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) The Hilbert Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Understanding the Analytic Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Advantage of GPU Parallelization when Performing Hilbert Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Extracting the Real Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iv) Extracting the Hilbert Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (v) Extracting the Instantaneous Phase Angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vi) Example Plots of Hilbert Envelopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (vii) Example Plots of Signal Phase Angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Generation of Simple Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Performing the Bulk Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualizations of Resulting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Individual Frequency Spectrums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Heatmaps of Frequency Spectrums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Audio Signal Processing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reduction of the Noise Floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Issues Caused by Noise Floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Method of Noise Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Noise Reduction Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reduction of Momentary Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Issued Caused by Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Means of Detecting Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Peak Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Calcuating the Click-Sensitive Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Reducing Magnitude around Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retuction Of Transient Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Introduction to Signal Transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Issues Caused by Transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Calculating the Transient-Sensitive Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Reducing Magnitude around Transients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Segmentation of Audio into Normalized Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Generation of Filtered Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Generation of Filtered Time-Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง V. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Filtered Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง V. Advanced Model Training and Species Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Training a Convolutional Neural Network (CNN) Against the Filtered Time-Frequency Representation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Model Training with Species as a Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Training a Convolutional Neural Network (CNN) with Species as a Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ยง VI. Practical Application and Use of Given Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Recreating the Audio Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Making the Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Testing Against the Given Test Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
